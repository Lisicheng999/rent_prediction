{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用深度学习方法进行建模\n",
    "#### LSTM （长短期记忆网络）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Address    Rent  Beds  Baths  Type  Area_in_sqft  Rent_per_sqft  \\\n",
      "0     3905  124000     3      4     0          1785      69.467787   \n",
      "1     4219  140000     3      4     0          1422      98.452883   \n",
      "\n",
      "   Rent_category  Frequency  Furnishing  Purpose  Posted_date  \\\n",
      "0              2          0           1        0          503   \n",
      "1              2          0           1        0          504   \n",
      "\n",
      "   Age_of_listing_in_days  Location  City   Latitude  Longitude  \n",
      "0                      45       135     0  24.493598  54.407841  \n",
      "1                      44       353     0  24.494022  54.607372  \n",
      "Address                     int64\n",
      "Rent                        int64\n",
      "Beds                        int64\n",
      "Baths                       int64\n",
      "Type                        int64\n",
      "Area_in_sqft                int64\n",
      "Rent_per_sqft             float64\n",
      "Rent_category               int64\n",
      "Frequency                   int64\n",
      "Furnishing                  int64\n",
      "Purpose                     int64\n",
      "Posted_date                 int64\n",
      "Age_of_listing_in_days      int64\n",
      "Location                    int64\n",
      "City                        int64\n",
      "Latitude                  float64\n",
      "Longitude                 float64\n",
      "dtype: object\n",
      "shape: (73023, 17)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data=pd.read_csv(\"encoded_dataset.csv\")\n",
    "data.dropna(inplace=True)\n",
    "print(data.head(2))\n",
    "# data\n",
    "\n",
    "# 查看数据类型\n",
    "print(data.dtypes)\n",
    "print(\"shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Address  Beds  Baths  Type  Area_in_sqft  Rent_per_sqft  Rent_category  \\\n",
      "0     3905     3      4     0          1785      69.467787              2   \n",
      "1     4219     3      4     0          1422      98.452883              2   \n",
      "\n",
      "   Frequency  Furnishing  Purpose  Posted_date  Age_of_listing_in_days  \\\n",
      "0          0           1        0          503                      45   \n",
      "1          0           1        0          504                      44   \n",
      "\n",
      "   Location  City   Latitude  Longitude  \n",
      "0       135     0  24.493598  54.407841  \n",
      "1       353     0  24.494022  54.607372  \n",
      "*************************************\n",
      "0    124000\n",
      "1    140000\n",
      "Name: Rent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 准备特征和目标变量\n",
    "X = data.drop(['Rent'],  axis=1)\n",
    "y = data['Rent']\n",
    "# # 根据分类剔除分类为1的维度\n",
    "# X = X.drop(['Frequency', 'Purpose'], axis=1)\n",
    "print(X.head(2))\n",
    "print(\"*************************************\")\n",
    "print(y.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.47687015  0.5381336   0.83981347 -0.53658716 -0.08421462 -0.28621261\n",
      "   1.20988169  0.          0.52157908  0.          0.43296941 -0.40208572\n",
      "  -0.84502384 -1.18090496 -0.74704453 -0.98711226]\n",
      " [ 1.72925982  0.5381336   0.83981347 -0.53658716 -0.20618471  0.14882219\n",
      "   1.20988169  0.          0.52157908  0.          0.44833278 -0.41600607\n",
      "   1.69240706 -1.18090496 -0.7463     -0.68188639]]\n",
      "(73023, 16)\n",
      "(73023,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 使用标准归一化器\n",
    "standard_scaler = StandardScaler()\n",
    "X_scaled = standard_scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled[:2])\n",
    "print(X_scaled.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73023, 1)\n",
      "[[-0.07906335]\n",
      " [-0.02715964]]\n"
     ]
    }
   ],
   "source": [
    "# 归一化正确，将y也归一化处理\n",
    "y_from_to_frame = y.to_frame()\n",
    "print(y_from_to_frame.shape)\n",
    "y_scaled = standard_scaler.fit_transform(y_from_to_frame)\n",
    "print(y_scaled[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26395294 -1.3711756  -1.0110441  -0.5365872  -0.46558115 -0.47449508\n",
      "  -0.0099224   0.          0.5215791   0.          0.5251496  -0.48560783\n",
      "  -1.694714   -1.180905   -0.78964376 -1.0774182 ]\n",
      " [ 1.6480771  -1.3711756  -0.3940916  -0.5365872  -0.52505416  1.6856266\n",
      "   1.2098817   0.         -1.9172548   0.          0.01815861 -0.02623618\n",
      "  -0.12336918  0.35166693  0.4750966   0.3238181 ]\n",
      " [-1.1756327   0.5381336   0.8398135  -0.5365872   0.46280274 -0.2734313\n",
      "  -1.2297266   0.          0.5215791   0.         -1.8100816   1.6302859\n",
      "  -1.0196177  -1.180905   -0.7605115  -0.6917267 ]]\n",
      "训练集特征张量大小 torch.Size([65720, 16])\n",
      "训练集目标张量大小 torch.Size([65720, 1])\n",
      "测试集目标张量大小: torch.Size([7303, 16])\n",
      "测试集目标张量大小: torch.Size([7303, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.1, random_state=42) # 批量归一化\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "print(X_train_tensor[:3].numpy())\n",
    "print(\"训练集特征张量大小\", X_train_tensor.size())\n",
    "print(\"训练集目标张量大小\", y_train_tensor.size())\n",
    "print(\"测试集目标张量大小:\", X_test_tensor.size())\n",
    "print(\"测试集目标张量大小:\", y_test_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置硬件\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "# 将数据也送到GPU\n",
    "X_train_tensor_gpu = X_train_tensor.to(device)\n",
    "X_test_tensor_gpu = X_test_tensor.to(device)\n",
    "y_train_tensor_gpu = y_train_tensor.to(device)\n",
    "y_test_tensor_gpu = y_test_tensor.to(device)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# 创建数据加载器，指定批量大小\n",
    "batch_size =128\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor_gpu, y_train_tensor_gpu)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor_gpu, y_test_tensor_gpu)\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)  # 关闭打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建LSTM\n",
    "class RentPredictor_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(RentPredictor_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状是 (batch_size, feature_dim)\n",
    "        # 由于我们的数据是二维的，我们需要添加一个序列长度维度\n",
    "        x = x.unsqueeze(1)  # 现在形状变为 (batch_size, 1, feature_dim)\n",
    "        \n",
    "        # 初始化隐藏状态和细胞状态\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # 前向传播LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # 解码最后一个时间步的隐藏状态\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 定义LSTM模型的参数\n",
    "hidden_dim = 128  # 隐藏层的维度\n",
    "num_layers = 4   # LSTM层的数量\n",
    "output_dim = 1  # 输出的维度\n",
    "\n",
    "print(X_train_tensor.shape[1])\n",
    "# 实例化模型\n",
    "model = RentPredictor_LSTM(input_dim=X_train_tensor.shape[1], \n",
    "\t\t\t\t\t\t   hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "model.to(device)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "# 定义优化器\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "# 定义训练轮数\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# 训练模式\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_data_loader:\n\u001b[0;32m     14\u001b[0m         X_batch, y_batch \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     15\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 存储训练log\n",
    "loss_values = []\n",
    "MSE_values = []\n",
    "RMSE_values = []\n",
    "sMAPE_values = []\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    # 训练模式\n",
    "    model.train()\n",
    "    for batch in train_data_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_values.append(loss.item())\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "    \n",
    "    # # 评估模式\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     y_val_pred = model(X_test_tensor.to(device))\n",
    "    #     # 将预测结果从GPU复制到CPU\n",
    "    #     y_val_pred_cpu = y_val_pred.to('cpu')\n",
    "    #     # 将预测结果转换为Numpy数组\n",
    "    #     y_val_pred_numpy = y_val_pred_cpu.numpy()\n",
    "    #     # 安全地使用y_val_pred_numpy作为mean_squared_error的输入\n",
    "    #     MSE_lr = mean_squared_error(y_test, y_val_pred_numpy)\n",
    "    #     RMSE_lr = np.sqrt(MSE_lr)\n",
    "        \n",
    "\t# \t# 计算sMAPE\n",
    "    #     abs_diff = np.abs(y_test - y_val_pred_numpy)\n",
    "    #     avg_abs = (np.abs(y_test) + np.abs(y_val_pred_numpy)) / 2\n",
    "    #     sMAPE = np.mean((abs_diff / avg_abs) * 100)\n",
    "        \n",
    "    #     MSE_values.append(MSE_lr)\n",
    "    #     RMSE_values.append(RMSE_lr)\n",
    "    #     sMAPE_values.append(sMAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0: torch.Size([512, 16])\n",
      "lstm.weight_hh_l0: torch.Size([512, 128])\n",
      "lstm.bias_ih_l0: torch.Size([512])\n",
      "lstm.bias_hh_l0: torch.Size([512])\n",
      "lstm.weight_ih_l1: torch.Size([512, 128])\n",
      "lstm.weight_hh_l1: torch.Size([512, 128])\n",
      "lstm.bias_ih_l1: torch.Size([512])\n",
      "lstm.bias_hh_l1: torch.Size([512])\n",
      "lstm.weight_ih_l2: torch.Size([512, 128])\n",
      "lstm.weight_hh_l2: torch.Size([512, 128])\n",
      "lstm.bias_ih_l2: torch.Size([512])\n",
      "lstm.bias_hh_l2: torch.Size([512])\n",
      "lstm.weight_ih_l3: torch.Size([512, 128])\n",
      "lstm.weight_hh_l3: torch.Size([512, 128])\n",
      "lstm.bias_ih_l3: torch.Size([512])\n",
      "lstm.bias_hh_l3: torch.Size([512])\n",
      "fc.weight: torch.Size([1, 128])\n",
      "fc.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# 查看模型参数\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}:\", param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 471169 parameters.\n"
     ]
    }
   ],
   "source": [
    "# 获取模型的所有参数\n",
    "parameters = list(model.parameters())\n",
    "\n",
    "# 将所有参数打包成一个向量\n",
    "parameters_vector = torch.cat([p.view(-1) for p in parameters])\n",
    "\n",
    "# 计算参数量\n",
    "num_parameters = parameters_vector.numel()\n",
    "\n",
    "print(f\"The model has {num_parameters} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.0357492582323161\n",
      "Test RMSE: 0.18907474244942424\n",
      "Test R²: 0.9623569206365389\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val_pred = model(X_test_tensor.to(device))\n",
    "    \n",
    "# 将预测结果从GPU复制到CPU\n",
    "y_val_pred_cpu = y_val_pred.to('cpu')\n",
    "\n",
    "# 将预测结果转换为Numpy数组\n",
    "y_val_pred_numpy = y_val_pred_cpu.numpy()\n",
    "\n",
    "# 安全地使用y_val_pred_numpy作为mean_squared_error的输入\n",
    "MSE_lr = mean_squared_error(y_test, y_val_pred_numpy)\n",
    "RMSE_lr = np.sqrt(MSE_lr)\n",
    "r2_lr = r2_score(y_test, y_val_pred_numpy)\n",
    "\n",
    "# 打印指标\n",
    "print(f'Test MSE: {MSE_lr}')\n",
    "print(f'Test RMSE: {RMSE_lr}')\n",
    "print(f'Test R²: {r2_lr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反归一化后的预测结果: [[226235.95]\n",
      " [ 45659.05]\n",
      " [ 94983.11]\n",
      " [135053.38]\n",
      " [134391.44]]\n",
      "反归一化后的实际目标值: [[230000.   ]\n",
      " [ 45000.004]\n",
      " [ 90000.   ]\n",
      " [135000.   ]\n",
      " [135000.   ]]\n"
     ]
    }
   ],
   "source": [
    "# 选择测试集中的几个样本\n",
    "X_test_sample = X_test_tensor_gpu[:5]  # 选择前5个样本\n",
    "y_test_sample = y_test_tensor_gpu[:5]\n",
    "\n",
    "# 使用模型进行预测\n",
    "with torch.no_grad():  # 确保不计算梯度\n",
    "    y_pred_sample = model(X_test_sample)\n",
    "\n",
    "# 将预测结果和实际目标值转换为 NumPy 数组\n",
    "y_pred_sample_np = y_pred_sample.cpu().numpy()\n",
    "y_test_sample_np = y_test_sample.cpu().numpy()\n",
    "\n",
    "# 反归一化\n",
    "y_pred_original = standard_scaler.inverse_transform(y_pred_sample_np)\n",
    "y_test_original = standard_scaler.inverse_transform(y_test_sample_np)\n",
    "\n",
    "# 打印反归一化后的预测结果和实际目标值\n",
    "print('反归一化后的预测结果:', y_pred_original)\n",
    "print('反归一化后的实际目标值:', y_test_original)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
